{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a049d0c",
   "metadata": {},
   "source": [
    "# AdaBoost vs GBM.\n",
    "\n",
    "## 5 Diferenças entre AdaBoost e GBM:\n",
    "\n",
    "Diferença | AdaBoost | GBM\n",
    "--- | --- | ---\n",
    "Tipo de modelo base | Normalmente usa árvores de decisão rasas (stumps) como modelos base. | Pode usar árvores de decisão mais profundas ou outros modelos como base.\n",
    "Peso dos modelos | Cada modelo é ponderado com base em sua precisão, e os erros são corrigidos nas iterações subsequentes. | Cada modelo é ajustado para corrigir os erros residuais do modelo anterior.\n",
    "Foco no erro | Dá mais peso às amostras que foram classificadas incorretamente. | Foca na minimização do erro global, ajustando os resíduos.\n",
    "Complexidade do modelo | Geralmente resulta em modelos mais simples e rápidos de treinar. | Pode resultar em modelos mais complexos e demorados para treinar.\n",
    "Sensibilidade a ruído | Pode ser mais sensível a ruído nos dados devido ao foco em erros individuais. | Geralmente é mais robusto ao ruído, pois considera o erro global."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b86695",
   "metadata": {},
   "source": [
    "# Exemplo de uso do GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9b831ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e578762",
   "metadata": {},
   "source": [
    "# 5 Hyperparâmetros importantes do GBM:\n",
    "- n_estimators: Número de árvores a serem construídas no ensemble.\n",
    "- learning_rate: Taxa de aprendizado que controla o peso de cada árvore.\n",
    "- max_depth: Profundidade máxima das árvores individuais.\n",
    "- subsample: Fração de amostras a serem usadas para treinar cada árvore.\n",
    "- random_state: Semente para o gerador de números aleatórios, garantindo reprodutibilidade dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf124c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1, 'max_depth': 3, 'n_estimators': 150, 'subsample': 0.75}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.94525)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': [1, 3, 5],\n",
    "    'subsample': [0.5, 0.75, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "display(grid_search.best_params_)\n",
    "display(grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb8ace2",
   "metadata": {},
   "source": [
    "# Agora falando sobre o Stochastic Gradient Boosting Machine (GBM):\n",
    "\n",
    "Baseado em seu nome, sua maior diferença em relação ao Gradient Boosting Machine (GBM) tradicional é o uso de amostragem aleatória (stochastic) durante o treinamento. No Stochastic GBM, em cada iteração, uma amostra aleatória dos dados de treinamento é selecionada para treinar a próxima árvore. Isso ajuda a reduzir o overfitting e melhora a generalização do modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
